{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "print(\"execution started\")\r\n",
        "\r\n",
        "import mlflow\r\n",
        "import transformers\r\n",
        "\r\n",
        "\r\n",
        "modelname = 'meta-llama/Llama-2-7b'\r\n",
        "from transformers import GPT2LMHeadModel\r\n",
        "from transformers import GPT2Tokenizer\r\n",
        "\r\n",
        "print(\"before tokenizer\")\r\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(modelname)\r\n",
        "print(\"after tokenizer model\")\r\n",
        "model = GPT2LMHeadModel.from_pretrained(modelname)\r\n",
        "print(\"after automodel\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "basemodel = mlflow.transformers.log_model(\r\n",
        "        transformers_model={\"model\": model, \"tokenizer\": tokenizer },\r\n",
        "        task=\"text-generation\",\r\n",
        "        artifact_path=\"models/basemodel\",\r\n",
        "        input_example=[\"base model loaded directly from transformer's\"],\r\n",
        "        registered_model_name=\"Gpt-2-medium-base-model\"\r\n",
        "        )\r\n",
        "print(\"registered the model\")\r\n",
        "\r\n",
        "\r\n",
        "mlflow.transformers.save_model(\r\n",
        "    transformers_model={\"model\": model, \"tokenizer\": tokenizer },\r\n",
        "    path=\"transformersmodel\"\r\n",
        ")\r\n",
        "\r\n",
        "mlmodel = mlflow.transformers.load_model(\r\n",
        "    \"transformersmodel\"\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from datasets import load_dataset\r\n",
        "traindataset = load_dataset(\"cnn_dailymail\",\"3.0.0\", split=\"train\").shuffle().select(range(1000))\r\n",
        "testdataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\").shuffle().select(range(500))\r\n",
        "\r\n",
        "def tokenizer_function(examples):\r\n",
        "        return tokenizer(examples[\"article\"],text_target=examples[\"highlights\"],truncation=True,padding=True,max_length=128,return_tensors=\"pt\")\r\n",
        "\r\n",
        "tokenizer.pad_token = tokenizer.eos_token\r\n",
        "traindataset = traindataset.map(tokenizer_function,batched=True)\r\n",
        "testdataset = testdataset.map(tokenizer_function,batched=True)\r\n",
        "\r\n",
        "tokenizer.pad_token = tokenizer.eos_token\r\n",
        "traindataset = traindataset.map(tokenizer_function,batched=True)\r\n",
        "testdataset = testdataset.map(tokenizer_function,batched=True)\r\n",
        "\r\n",
        "traindataset = traindataset.remove_columns([\"article\",\"highlights\",\"id\"])\r\n",
        "testdataset = testdataset.remove_columns([\"article\",\"highlights\",\"id\"])\r\n",
        "\r\n",
        "import evaluate\r\n",
        "metrics = evaluate.load(\"perplexity\")\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "def compute_metrics(eval_period):\r\n",
        "        print(eval_period) \r\n",
        "        logits, labels = eval_period\r\n",
        "        predictions = np.argmax(logits,x_axis = 1)\r\n",
        "        return metrics.compute(predictions = predictions, references = labels)\r\n",
        "\r\n",
        "\r\n",
        "import os\r\n",
        "os.environ[\"DISABLE_MLFLOW_INTEGRATION\"] = \"TRUE\"\r\n",
        "\r\n",
        "from transformers import TrainingArguments,Trainer\r\n",
        "\r\n",
        "Training_args = TrainingArguments(\r\n",
        "        num_train_epochs = 1,\r\n",
        "        output_dir= \"output\",\r\n",
        "        save_strategy=\"epoch\"\r\n",
        "    )\r\n",
        "\r\n",
        "trainer = Trainer(\r\n",
        "        model = mlmodel.model,\r\n",
        "        tokenizer=tokenizer,\r\n",
        "        args = Training_args,\r\n",
        "        compute_metrics=compute_metrics,\r\n",
        "        train_dataset = traindataset,\r\n",
        "        eval_dataset = testdataset,\r\n",
        "        \r\n",
        "    )\r\n",
        "\r\n",
        "result = trainer.train()\r\n",
        "\r\n",
        "print(result)\r\n",
        "# trainedmodel = mlflow.transformers.log_model(\r\n",
        "#         transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer },\r\n",
        "#         task=\"text-generation\",\r\n",
        "#         artifact_path=\"models/trainedmodel\",\r\n",
        "#         input_example=[\"base model loaded directly from transformer's\"],\r\n",
        "#         registered_model_name=\"Gpt-2-medium-base-model\"\r\n",
        "#         )\r\n",
        "\r\n",
        "# mlflow.end_run()\r\n",
        "\r\n",
        "# print(\"trainer model got logged successfully\")\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "execution started\nbefore tokenizer\nafter tokenizer model\nafter automodel\nregistered the model\nDownloading and preparing dataset cnn_dailymail/3.0.0 to /home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de...\nDataset cnn_dailymail downloaded and prepared to /home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de. Subsequent calls will reuse this data.\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nDownloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 4.38MB/s]\nDownloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 37.3MB/s]\nDownloading (…)lve/main/config.json: 100%|██████████| 718/718 [00:00<00:00, 5.67MB/s]\nDownloading model.safetensors: 100%|██████████| 1.52G/1.52G [00:30<00:00, 50.1MB/s]\nDownloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 832kB/s]\n/tmp/ipykernel_12242/2363559458.py:19: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.30.2``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n  basemodel = mlflow.transformers.log_model(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/mlflow/models/model.py:572: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.30.2``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\nDownloading (…)solve/main/README.md: 100%|██████████| 11.9k/11.9k [00:00<00:00, 55.3MB/s]\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nRegistered model 'Gpt-2-medium-base-model' already exists. Creating a new version of this model...\n2023/07/26 07:55:03 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: Gpt-2-medium-base-model, version 15\nCreated version '15' of model 'Gpt-2-medium-base-model'.\n/tmp/ipykernel_12242/2363559458.py:29: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.30.2``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n  mlflow.transformers.save_model(\n/tmp/ipykernel_12242/2363559458.py:34: FutureWarning: The 'transformers' MLflow Models integration is known to be compatible with the following package version ranges: ``4.25.1`` -  ``4.30.2``. MLflow Models integrations with transformers may not succeed when used with package versions outside of this range.\n  mlmodel = mlflow.transformers.load_model(\nLoading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.89s/it]\nDownloading builder script: 100%|██████████| 8.33k/8.33k [00:00<00:00, 8.83MB/s]\nDownloading metadata: 100%|██████████| 9.88k/9.88k [00:00<00:00, 7.77MB/s]\nDownloading readme: 100%|██████████| 15.1k/15.1k [00:00<00:00, 10.9MB/s]\nDownloading data files:   0%|          | 0/5 [00:00<?, ?it/s]\nDownloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]\u001b[A\nDownloading data:   3%|▎         | 5.45M/159M [00:00<00:02, 54.5MB/s]\u001b[A\nDownloading data:  10%|▉         | 15.2M/159M [00:00<00:01, 80.0MB/s]\u001b[A\nDownloading data:  16%|█▌        | 25.2M/159M [00:00<00:01, 88.8MB/s]\u001b[A\nDownloading data:  22%|██▏       | 35.0M/159M [00:00<00:01, 92.5MB/s]\u001b[A\nDownloading data:  28%|██▊       | 44.9M/159M [00:00<00:01, 94.8MB/s]\u001b[A\nDownloading data:  35%|███▍      | 54.9M/159M [00:00<00:01, 96.5MB/s]\u001b[A\nDownloading data:  41%|████      | 64.7M/159M [00:00<00:00, 97.3MB/s]\u001b[A\nDownloading data:  47%|████▋     | 74.5M/159M [00:00<00:00, 97.3MB/s]\u001b[A\nDownloading data:  53%|█████▎    | 84.2M/159M [00:00<00:00, 92.6MB/s]\u001b[A\nDownloading data:  59%|█████▉    | 94.0M/159M [00:01<00:00, 94.2MB/s]\u001b[A\nDownloading data:  66%|██████▌   | 104M/159M [00:01<00:00, 95.8MB/s] \u001b[A\nDownloading data:  72%|███████▏  | 114M/159M [00:01<00:00, 96.6MB/s]\u001b[A\nDownloading data:  78%|███████▊  | 124M/159M [00:01<00:00, 97.4MB/s]\u001b[A\nDownloading data:  84%|████████▍ | 133M/159M [00:01<00:00, 95.6MB/s]\u001b[A\nDownloading data:  90%|█████████ | 143M/159M [00:01<00:00, 96.7MB/s]\u001b[A\nDownloading data: 100%|██████████| 159M/159M [00:01<00:00, 94.5MB/s]\u001b[A\nDownloading data files:  20%|██        | 1/5 [00:02<00:08,  2.03s/it]\nDownloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]\u001b[A\nDownloading data:   2%|▏         | 6.28M/376M [00:00<00:05, 62.8MB/s]\u001b[A\nDownloading data:   4%|▍         | 15.9M/376M [00:00<00:04, 82.3MB/s]\u001b[A\nDownloading data:   7%|▋         | 25.3M/376M [00:00<00:03, 87.8MB/s]\u001b[A\nDownloading data:   9%|▉         | 34.1M/376M [00:00<00:03, 86.1MB/s]\u001b[A\nDownloading data:  12%|█▏        | 43.7M/376M [00:00<00:03, 89.8MB/s]\u001b[A\nDownloading data:  14%|█▍        | 53.6M/376M [00:00<00:03, 92.9MB/s]\u001b[A\nDownloading data:  17%|█▋        | 63.4M/376M [00:00<00:03, 94.6MB/s]\u001b[A\nDownloading data:  19%|█▉        | 73.3M/376M [00:00<00:03, 95.8MB/s]\u001b[A\nDownloading data:  22%|██▏       | 83.1M/376M [00:00<00:03, 96.5MB/s]\u001b[A\nDownloading data:  25%|██▍       | 92.8M/376M [00:01<00:03, 93.8MB/s]\u001b[A\nDownloading data:  27%|██▋       | 102M/376M [00:01<00:02, 94.8MB/s] \u001b[A\nDownloading data:  30%|██▉       | 112M/376M [00:01<00:02, 95.7MB/s]\u001b[A\nDownloading data:  32%|███▏      | 122M/376M [00:01<00:02, 96.4MB/s]\u001b[A\nDownloading data:  35%|███▌      | 132M/376M [00:01<00:02, 96.0MB/s]\u001b[A\nDownloading data:  38%|███▊      | 141M/376M [00:01<00:02, 95.0MB/s]\u001b[A\nDownloading data:  40%|████      | 151M/376M [00:01<00:02, 96.0MB/s]\u001b[A\nDownloading data:  43%|████▎     | 161M/376M [00:01<00:02, 96.3MB/s]\u001b[A\nDownloading data:  45%|████▌     | 171M/376M [00:01<00:02, 97.0MB/s]\u001b[A\nDownloading data:  48%|████▊     | 181M/376M [00:01<00:02, 97.3MB/s]\u001b[A\nDownloading data:  51%|█████     | 190M/376M [00:02<00:01, 93.8MB/s]\u001b[A\nDownloading data:  53%|█████▎    | 200M/376M [00:02<00:01, 94.9MB/s]\u001b[A\nDownloading data:  56%|█████▌    | 210M/376M [00:02<00:01, 95.8MB/s]\u001b[A\nDownloading data:  58%|█████▊    | 220M/376M [00:02<00:01, 96.5MB/s]\u001b[A\nDownloading data:  61%|██████    | 230M/376M [00:02<00:01, 97.3MB/s]\u001b[A\nDownloading data:  64%|██████▎   | 239M/376M [00:02<00:01, 97.3MB/s]\u001b[A\nDownloading data:  66%|██████▋   | 249M/376M [00:02<00:01, 95.9MB/s]\u001b[A\nDownloading data:  69%|██████▉   | 259M/376M [00:02<00:01, 95.9MB/s]\u001b[A\nDownloading data:  71%|███████▏  | 268M/376M [00:02<00:01, 96.0MB/s]\u001b[A\nDownloading data:  74%|███████▍  | 278M/376M [00:02<00:01, 96.0MB/s]\u001b[A\nDownloading data:  77%|███████▋  | 288M/376M [00:03<00:00, 96.5MB/s]\u001b[A\nDownloading data:  79%|███████▉  | 297M/376M [00:03<00:00, 96.6MB/s]\u001b[A\nDownloading data:  82%|████████▏ | 307M/376M [00:03<00:00, 97.5MB/s]\u001b[A\nDownloading data:  84%|████████▍ | 317M/376M [00:03<00:00, 92.9MB/s]\u001b[A\nDownloading data:  87%|████████▋ | 326M/376M [00:03<00:00, 93.0MB/s]\u001b[A\nDownloading data:  89%|████████▉ | 336M/376M [00:03<00:00, 94.7MB/s]\u001b[A\nDownloading data:  92%|█████████▏| 346M/376M [00:03<00:00, 95.6MB/s]\u001b[A\nDownloading data:  95%|█████████▍| 356M/376M [00:03<00:00, 94.3MB/s]\u001b[A\nDownloading data:  97%|█████████▋| 365M/376M [00:03<00:00, 95.0MB/s]\u001b[A\nDownloading data: 100%|██████████| 376M/376M [00:03<00:00, 94.7MB/s]\u001b[A\nDownloading data files:  40%|████      | 2/5 [00:06<00:10,  3.45s/it]\nDownloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]\u001b[A\nDownloading data:  82%|████████▏ | 10.1M/12.3M [00:00<00:00, 101MB/s]\u001b[A\nDownloading data: 20.2MB [00:00, 98.0MB/s]                           \u001b[A\nDownloading data: 30.0MB [00:00, 95.6MB/s]\u001b[A\nDownloading data: 46.4MB [00:00, 95.4MB/s]\u001b[A\nDownloading data files:  60%|██████    | 3/5 [00:09<00:06,  3.39s/it]\nDownloading data: 2.43MB [00:00, 60.0MB/s]                  \u001b[A\nDownloading data files:  80%|████████  | 4/5 [00:10<00:02,  2.25s/it]\nDownloading data: 2.11MB [00:00, 59.9MB/s]                  \u001b[A\nDownloading data files: 100%|██████████| 5/5 [00:10<00:00,  2.14s/it]\nFound cached dataset cnn_dailymail (/home/azureuser/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n100%|██████████| 1/1 [00:05<00:00,  5.99s/ba]\n100%|██████████| 1/1 [00:02<00:00,  2.61s/ba]\n100%|██████████| 1/1 [00:04<00:00,  4.40s/ba]\n100%|██████████| 1/1 [00:02<00:00,  2.26s/ba]\nDownloading builder script: 100%|██████████| 8.46k/8.46k [00:00<00:00, 5.02MB/s]\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 04:25, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric train_runtime:\n267.4291\nAttempted to log scalar metric train_samples_per_second:\n3.739\nAttempted to log scalar metric train_steps_per_second:\n0.467\nAttempted to log scalar metric total_flos:\n232175173632000.0\nAttempted to log scalar metric train_loss:\n4.2668056640625\nAttempted to log scalar metric epoch:\n1.0\nTrainOutput(global_step=125, training_loss=4.2668056640625, metrics={'train_runtime': 267.4291, 'train_samples_per_second': 3.739, 'train_steps_per_second': 0.467, 'total_flos': 232175173632000.0, 'train_loss': 4.2668056640625, 'epoch': 1.0})\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1690358644390
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_tokenizer = {\"model\":model, \"tokenizer\":tokenizer}"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690365258667
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_tokenizer[\"model\"]\r\n",
        "tokenizer = model_tokenizer[\"tokenizer\"]\r\n",
        "inputs = tokenizer(\"Hello, my dog is \", \"The movie was \", return_tensors=\"pt\")\r\n",
        "output = model(**inputs)\r\n",
        "predictions = torch.nn.functional.softmax(output.logits, dim=-1)\r\n",
        "print(f'Predicted class: {predictions}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Predicted class: tensor([[[7.3768e-02, 3.5655e-02, 3.0460e-05,  ..., 1.1749e-09,\n          8.6480e-09, 1.5610e-03],\n         [4.2479e-04, 1.0622e-04, 1.2661e-05,  ..., 2.4750e-07,\n          2.7505e-07, 3.5757e-04],\n         [8.7142e-05, 1.8022e-05, 4.4241e-07,  ..., 3.9378e-07,\n          5.4928e-08, 3.2181e-06],\n         ...,\n         [1.4669e-02, 4.7507e-03, 9.1801e-06,  ..., 1.1602e-08,\n          3.2272e-07, 6.4735e-04],\n         [2.4840e-04, 9.0125e-05, 7.3380e-06,  ..., 2.6484e-09,\n          2.0246e-07, 5.6557e-05],\n         [1.7301e-03, 5.5349e-03, 6.0197e-04,  ..., 8.7947e-10,\n          1.1176e-07, 2.4396e-04]]], grad_fn=<SoftmaxBackward0>)\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690365812434
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}